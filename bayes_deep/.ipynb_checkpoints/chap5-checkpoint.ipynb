{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ベイズニューラルネットワークモデルの近似推論法  \n",
    "バッチ学習によるニューラルネットワークの基本的な学習手法  \n",
    "入力データ$X = \\{x_1, ..., x_N\\}$  \n",
    "観測データ$Y=\\{y_1,...,y_N\\}$  \n",
    "パラメータ$W$\n",
    "$$\n",
    "p(y, W|X) = p(W) \\prod_{n=1}^N p(y_n |x_n, W) \\tag{1}\n",
    "$$\n",
    "とおく  \n",
    "$x_n\\in \\mathbb{R}^{H_0}$から$y_n \\in \\mathbb{R}^D$を予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y_n|x_n, W) = N(y_n|f(x_n;W), \\sigma^2_yI) \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x_n;W)$がニューラルネットワーク  \n",
    "ここでは分散$\\sigma^2_y$は固定のノイズ項\n",
    "$$\n",
    "f(x_n;W) = \\sum_{h_1=1}^{H_1} w_{d,h1}^{(2)}\\phi \\left( \\sum_{h_0=1}^{H_0} w_{h_1, h_0}^{(1)} x_{n,h_0} \\right) \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データが与えられた後のパラメータの事後分布をべいず推論で計算するのでパラメータに事前分布を設定する必要がある  \n",
    "ここでは各重みパラメータを$w\\in W$とし，それぞれに独立なガウス分布を与える  \n",
    "$$\n",
    "p(w) = N(w|0,\\sigma_w^2) \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 ラプラス近似による学習  \n",
    "式(1)のニューラルネットワークモデルに対するラプラス近似による学習と予測を導出する  \n",
    "簡単のためD=1とする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. モデルの事後分布のMAP推定値を最適化により求める　　\n",
    "$$\n",
    "p(W|Y,X) \\propto p(W)p(Y|X,W) \\tag{5}\\\\\n",
    "$$\n",
    "局所最適解$W_{MAP}$の更新は対数事後分布の勾配を利用して\n",
    "$$\n",
    "W_{new} = W_{old} + \\alpha \\nabla _w \\log P(W|Y, W)|_{W=W_{old}} \\tag{6}\n",
    "$$\n",
    "2. 周辺をガウス分布によって求める  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ$w\\in W$の偏微分を計算すると   \n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} \\log p(W|Y,X) = - \\left\\{ \\frac{1}{\\sigma_y^2}\\frac{\\partial}{\\partial w} E(W) + \\frac{1}{\\sigma_w^2}\\frac{\\partial}{\\partial w} \\Omega_{L2} \\right\\} \\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Omega_{L2}$は正規化項で，容易に微分できる  \n",
    "$E(W)$はニューラルネットワークの誤差関数  \n",
    "\n",
    "ラプラス近似で計算される近似が近似事後分布を\n",
    "$$\n",
    "q(W) = N(W|W_{MAP}, \\{\\Lambda (W_{MAP})\\}^{-1}) \\tag{9}\n",
    "$$\n",
    "とおく  \n",
    "精度行列$\\Lambda$は\n",
    "\\begin{eqnarray}\n",
    "    \\Lambda = -\\nabla^2_W \\log p(W|Y,X) \\\\\n",
    "    = \\frac{1}{\\sigma_w^2} I + \\frac{1}{\\sigma_y^2}H \\tag{10}\n",
    "\\end{eqnarray}\n",
    "$H$はヘッセ行列で厳密計算あるいは近似計算によって求められる  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.2. 予測分布の近似  \n",
    "$$\n",
    "p(y_*|x_*, Y, X) \\approx  \\int p(y_* | x_*, W) q(W)dW \\tag{11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータWはq(W)としてガウス分布で近似しているが$p(y_*|x_*, W)$の中にニューラルネットワークが含まれているので解析的に計算できない　　\n",
    "→ニューラルネットワークの線形近似を行う  \n",
    "パラメータの事後分布の密度がMAP推定値の周辺に集中していて，その範囲においてニューラルネットワークの間数値$f(x_*, W)$がWの線形関数でよく近似できるという仮定をおく  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "f(x_* ; W) \\approx f(x_* ; W_{MAP}) + g^T(W-W_{MAP}) \\tag{12} \\\\\n",
    "g = \\nabla wf(x_*;W)|_{W=W_{MAP}} \\tag{13}\n",
    "\\end{eqnarray}\n",
    "テイラー展開でWの関数$f(x_*;W)$を$W_{MAP}$周りで一次近似したもの  \n",
    "gは関数の勾配を$W_{MAP}$で評価したもの  \n",
    "この近似を使えばニューラルネットワークの非線形性がなくなるので解析的に計算できるようになる  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "    p(y_*|x_*, Y, X ) \\approx \\int p(y_* | x_*, W)q(W)d(W) \\\\\n",
    "    \\approx \\int N(y_*|f(x_* ; W_{MAP} ) + g^T(W - W_{MAP} ), \\sigma^2_y)N(W|W_{MAP}, \\{\\Lambda (W_{MAP})\\}^{-1})dW\\\\\n",
    "    = N(y_* | f(x_*; W_{MAP}), \\sigma^2(x_*)) \\tag{14} \\\\\n",
    "    ただし \\sigma^2(x_*) = \\sigma^2_y + g^T\\{\\Lambda (W_{MAP})\\}^{-1}g\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 ハミルトニアンモンテカルロ法による学習  \n",
    "ハミルトニアンモンテカルロ法を利用したベイズミューラルネットワークの事後分布からのサンプリング  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ハミルトニアンモンテカルロ法は対数事後分布がサンプリングしたい変数に関して微分可能であれば適用できる  \n",
    "離散変数をパラメータとしてもたないニューラルネットワークではハミルトニアンモンテカルロ法に必要な微分情報の計算には誤差逆伝播ほうが使える  \n",
    "計算時間さえ十分に確保していれば理論的には真の事後分布からのサンプルが得られる  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(5)の正規化されていない事後分布を利用すると対応するポテンシャルエネルギーは\n",
    "$$\n",
    "U(W) = - \\{\\log p(Y|X,W) + \\log p(W)\\} \\tag{16}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リープフロッグ法を使うためにはポテンシャルエネルギーの微分が必要  \n",
    "→正則化項を付け加えたコスト関数の微分と等価なので誤差逆伝播法による勾配計算が利用できる  \n",
    "ハミルトニアンモンテカルロ法では予測の不確実性がサンプリングに基づく手法で得られる複数の関数のサンプリングから表現される  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.2 ハイパーパラメータの推論\n",
    "重みパラメータWの事前分布を支配する$\\sigma^2_w$や観測モデルのノイズパラメータ$\\sigma^2_y$などのハイパーパラメータに対して事前分布を設定する  \n",
    "パラメータWは分散$\\sigma^2_w$を持つガウス分布に従って決定される\n",
    "$$\n",
    "p(\\gamma_w) = Gam(\\gamma_w|a_w,b_w) \\tag{17} \\\\\n",
    "ただし\\gamma_w = \\sigma^{-2}_w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "観測ノイズも式(17)同様にガンマ分布を設定する  \n",
    "これらの精度パラメータの事前分布を導入した場合のモデルは  \n",
    "$$\n",
    "p(Y,W,\\gamma_w, \\gamma_y | X) = p(\\gamma_w)p(\\gamma_y)p(W|\\gamma_w) \\prod_{n=1}^N p(y_n|x_n, W, \\gamma_y) \\tag{19}\n",
    "$$\n",
    "従って事後分布全体は\n",
    "$$\n",
    "p(W,\\gamma_w, \\gamma_y|Y,X) \\tag{20}\n",
    "$$となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求めたい確率変数($W, \\gamma_w, \\gamma_y$)を条件付き確立を用いて別々にサンプリングする  \n",
    "→他の２変数を既知として求めたいパラメータのみを確率変数として扱う  \n",
    "$$\n",
    "p(W|T,X, \\gamma_w, \\gamma_y) \\tag{21}\n",
    "$$\n",
    "これはパラメータの事後分布そのままなのでハミルトニアンモンテカルロ法を実行することでWのサンプルを得られる  \n",
    "$$\n",
    "p(\\gamma_w|Y,X,W,\\gamma_y) \\propto p(W|\\gamma_w)p(\\gamma_w) \\tag{22}\n",
    "$$\n",
    "$p(W|\\gamma_w)$はガウス分布，$\\gamma_w$の事前分布はガンマ分布なので22式もガンマ分布として解析的に求められる  \n",
    "\\begin{eqnarray}\n",
    "\\gamma_w ~ Gam(\\hat{a}_w, \\hat{b}_w) \\tag{23}\\\\\n",
    "\\hat{a}_w = a_w + \\frac{K_w}{2} \\tag{24}\\\\\n",
    "\\hat{b}_w = b_w + \\frac{1}{2} \\sum_{w\\in W} w^2 \\tag{25}\\\\\n",
    "ただし，K_wは重みパラメータWの総数\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(\\gamma_y|Y,X,W,\\gamma_w) \\propto p(\\gamma_y) \\prod_{n=1}^N p(y_n|x_n,W, \\gamma_y) \\tag{26}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と計算できる  \n",
    "観測モデル$p(Y|W,X,\\gamma_y)$がガウス分布，$\\gamma_y$の事前分布にはガンマ分布を用いているので解析的に分布の計算ができる "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
