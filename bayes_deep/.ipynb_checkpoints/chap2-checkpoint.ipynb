{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 線型回帰\n",
    "#### 2.1.1 最小二乗法による学習\n",
    "$$\n",
    "y = {\\bf w}^T \\phi (x) + \\epsilon \\\\\n",
    "= \\sum_{m=1}^M w_m \\phi_m (x) + \\epsilon\n",
    "$$\n",
    "誤差関数を最小化することが目標  \n",
    "誤差関数を二乗誤差にしてパラメータの学習をすることを最小二乗法という  \n",
    "#### 2.1.2 基底関数の選択  \n",
    "各関数$\\phi_i:\\mathbb{R}\\rightarrow\\mathbb{R}$\n",
    "を基底関数という  \n",
    "\"線形\"と呼ばれるのは基底関数のパラメータによる線形結合によって予測を行なっているため  \n",
    "#### 2.1.3 過剰適合と正則化  \n",
    "直感的には可能な限り次数の高い複雑な関数を使って回帰を実行する方が良いように思えるが  \n",
    "パラメータ数の多い複雑なモデルを数が十分でない学習データに当てはめると過剰適合を起こす\n",
    "#### 2.1.3.1 正則化項\n",
    "誤差関数$$E(w) = \\frac{1}{2} \\sum_{n=1}^N \\{ y_n - {\\bf w}^T {\\bf \\phi} (x_n) \\}^2$$に対してペナルティを与える項$$\\Omega_{L2}(w) = \\frac{1}{2}{\\bf w}^T{\\bf w}$$を追加し \n",
    "新たなコスト関数\n",
    "$$\n",
    "J(w) = E(w) + \\lambda \\Omega_{L2}(w)\\\\\n",
    "(\\lambda > 0)\n",
    "$$で定義された式のパラメータに関する最小化を行う\n",
    "- $\\lambda$はペナルティ項の強さを調整する  \n",
    "\n",
    "wの２次のペナルティ項を使った回帰の手法はリッジ回帰という\n",
    "- ペナルティ項によって${\\bf w}$の取りうる値に制限をかけて過剰適合を抑制する  \n",
    "- この時の$J(w)$の勾配も計算できるので最小化を解析的に行える\n",
    "- パラメータのとる値の大きさに関して二乗のスケールでペナルティが働くので特定のパラメータ$w_m$が極端に大きな値を持つことを抑制できる\n",
    "    - wの各パラメータがまんべんなく値を持つようになる\n",
    "\n",
    "L1正則化 $\\Omega_{L1}(w) = \\sum_{m=0}^{M-1} |w_m|$(絶対値で正則化) \n",
    "- パラメータのスケールに依存したペナルティにはなっていない\n",
    "    - 特定の値だけが大きくなっている疎な学習結果になりやすい  \n",
    "    \n",
    "LASSO(least absolute shrinkage and selection operator)  \n",
    "もよく使われるらしい(LASSOよくわからないので調べておく)\n",
    "#### 2.1.3.2 正則化による学習の問題点  \n",
    "1. 特徴量関数$\\phi$を予め固定する必要がある\n",
    "2. 実戦においてどの特徴量関数$\\phi$の選択がデータの傾向を表しているかの判断が難しい\n",
    "    - testデータに対する誤差から判断する(交差確認)という手はある\n",
    "        - 学習データが削減されてしまう、生成モデルや教師なし学習で誤差の計算は同じようにはできないなどの問題点\n",
    "        \n",
    "3. 正規化項の設定指針が不明瞭\n",
    "- ベイズ統計を用いることでより直観的な設計が可能になる(らしい)\n",
    "    - 「関数に滑らかさや周期性などの性質を直接与えることによって直観的なモデリングを行うことを可能にする」？\n",
    "        - 7章で扱う\n",
    "\n",
    "4. 予測の不確実性を表現できない\n",
    "- 不確実性を表す量が誤差最小化や正則化による学習にはない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ニューラルネットワーク\n",
    "\n",
    "データから基底関数自体も学習する\n",
    "- より広い関数の空間を考慮した回帰が行えるように  \n",
    "\n",
    "#### 2.2.1 順伝搬型ニューラルネットワーク  \n",
    "#### 2.2.1.1 2層の順伝搬型ニューラルネットワーク  \n",
    "タスク：多次元の入力${\\bf x_n}\\in\\mathbb{R}^{H_0}$から多次元のラベル${\\bf y_n}\\in\\mathbb{R}^D$を予測する  \n",
    "線形回帰で使った特徴量関数$\\phi$の内部にさらに線形回帰を構成するモデルを考える  \n",
    "$$\n",
    "y_{n,d} = \\sum_{h1=1}^{H_1} w_{d,h_1}^{(2)} \\phi \\left( \\sum_{h_0=1}^{H_0} w_{h_1, h_0}^{(1)}x_n,h_0\\right) + \\epsilon_{n,d}. \\\\\\\\\n",
    "この式はまとめて {\\bf y}_n = {\\bf W}^{(2)}{\\bf \\phi .}({\\bf W}^{(1)}{\\bf x}_n) + {\\bf \\epsilon}_n. とも表せるし\\\\\\\\\n",
    "分解して\n",
    "y_{n,d} = a_{n,d}^{(2)} + \\epsilon_{n,d}, \\\\\n",
    "a_{n,d}^{(2)} = \\sum_{h_1=1}^{H_1} w_{d,h_1}^{(2)} z_n, h_1, \\\\\n",
    " z_n, h_1 = \\phi (a_{n,h_1}^{(1)}),\\\\\n",
    " a_{n,h_1}^{(1)} = \\sum_{h_0=1}^{H_0} w_{h_1,h_0}^{(1)} x_n, h_0.とも表せる\n",
    "$$\n",
    "この時の  \n",
    "$w_{h_1,h_0}^{(1)}\\in\\mathbb{R}, w_{d,h_1}^{(2)}\\in\\mathbb{R}$をネットワークの重みパラメータ  \n",
    "$z_{n,h_1}\\in \\mathbb{R}$を隠れユニット  \n",
    "$a_{n,d}^{(2)}\\in\\mathbb{R}, a_{n,h_1}^{(1)}\\in\\mathbb{R}$など隠れユニットや入力値に対して重み付き和を取ったものを活性と呼ぶ  \n",
    "#### 2.2.1.2 さまざまな活性化関数\n",
    "ニューラルネットワークで用いられる基底関数を活性化関数と呼ぶ  \n",
    "- 主に非線型関数が用いられる\n",
    "$$\n",
    "{\\bfよく使われるもの}\\\\\n",
    "シグモイド関数 Sig(x) = \\frac{1}{1+e^{-x}}\\\\\\\\\n",
    "双曲線正接関数 Tanh(x)=\\frac{e^x - e^{-x}}{e^x+e^{-x}}\\\\ちなみに Tanh(x) = 2Sig(2x) -1 \\\\\\\\\n",
    "$$\n",
    "$$\n",
    "累積分布関数 \\Phi(x) = \\int_{-\\infty}^x {\\it N}(t|0,1) dt\\\\\\\\\n",
    "ガウスの誤差関数 Erf(x) = \\frac{2}{\\pi} \\int_0^x exp(-t^2)dt\\\\\\\\\n",
    "正規化線形関数 ReLU(x) = max(x,0)\\\\\n",
    "指数線形関数 ELU(x) = \\begin{eqnarray}\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "x, \\hspace{30pt}if\\, x>0\\\\\n",
    "a\\{e^x -1 \\}, \\hspace{10pt}if\\, x\\leq0\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "#### 2.2.1.3  ニューラルネットワークで表現される関数の例\n",
    "隠れユニットの数を増やすと表現力が増す  \n",
    "層数が2の順伝播型ニューラルネットワークでは隠れユニット層を増やすことで任意の連続関数を近似できる\n",
    "- ニューラルネットワークの普遍性定理  \n",
    "\n",
    "#### 2.2.1.4 複数層を持つ準伝播型ニューラルネットワーク  \n",
    "$$\n",
    "y_{n,d} = \\sum_{h_{L-1}=1}^{H_{L-1}} w_{d,h_{L-1}}^{(L)} \\phi \\left(\\sum_{h_{L-2}=1}^{H_{L-2}} w_{h_{L-1},h_{L-2}}^{(L-1)} ... \\phi \\left( \\sum_{h_0=1}^{H_0} w_{h_1, h_0}^{(1)} x_{n,h_0} \\right) ... \\right)\n",
    "$$\n",
    "の式のように拡張する事もできる  \n",
    "${\\bf リンク関数}$  \n",
    "従属変数が正規分布に従わない時、リンク関数でモデル化する事でモデルの正確さが向上する  \n",
    "\n",
    "#### 2.2.2 勾配降下法とニュートン・ラフトン法  \n",
    "#### 2.2.2.1 勾配降下法  \n",
    "誤差が最小となる解が解析的に計算できない  \n",
    "→計算機を使って数値的に最小値を求める→勾配降下法  \n",
    "誤差関数がユークリッド距離の近傍で最も急に増加する方向を求め,逆向きの方向にパラメータを少しだけ動かす事で最適化を行う\n",
    "$$\n",
    "{\\nabla _wE} = \\left(\\frac{{\\partial E(w)}}{{\\partial w_1}} ...\\frac{{\\partial E(w)}}{{\\partial w_M}} \\right)^T \\\\\\\\\n",
    "w_{new} = w_{old} - \\alpha {\\nabla _w E(w)} |_{w=w_{old}}.\n",
    "$$\n",
    "学習率$\\alpha$が大きいと学習が早い反面収束が安定しないが小さいと学習に時間がかかる  \n",
    "#### ニュートン・ラフトン法  \n",
    "二回微分で最適化を効率化する方法  \n",
    "まず最小化したい誤差関数を$\\bar{w}$周りのテイラー展開により二次近似する  \n",
    "$${\\small テイラー展開はf(x)の振る舞いがわかる点x_0からf(x)全体の振る舞いを予測する処理}$$\n",
    "$$\n",
    "E(w) \\approx \\bar{E}(w)\\\\\\\\\n",
    "= E(\\bar{w}) + {\\nabla}_wE(w)|^T_{w=\\bar{w}}(w-\\bar{w}) + \\frac{1}{2}(w-\\bar{w})^T{\\nabla _w^2E(w)}|_{w=\\bar{w}}(w-\\bar{w}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
